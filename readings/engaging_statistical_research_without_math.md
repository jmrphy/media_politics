### How to Engage Quantitative Social Science without *Really* Understanding It[^1]

Those of you with little or no background in statistics probably think you're not capable (or not allowed) to read, learn from, and even argue with published statistical research.[^2] You're wrong. Even if you think you're really bad at math and you've never taken a single statistics course, you're still capable of learning from and even arguing with published statistical research! This document explains a few simple facts about most published statistical research, and points out a few specific practices you can follow to read, learn from, and engage published statistical research without even *really* understanding the math behind it.[^3]

1. A simple verbal description of what "statistical modeling" means

Although there exists a wide variety of statistical modeling techniques, most published scholarly research in the social sciences uses a relatively small subset of this wide variety. For the most part, "statistical modeling" just means "telling a computer to run a quantitative dataset (think: an Excel spreadsheet) through a mathematical formula, and then printing numbers which measure how well the data fits into that equation." Some say "estimating a model", or "fitting a model," or colloquially "running a model" or "testing a model," but they all mean the same thing: just taking a well-known, mathematical formula pre-designed by statisticians for whatever type of data the analyst wants to study (in other words, just a bunch of *assumptions* about the data), fitting it to the data, and then analyzing to see if how well it fits! Typically, "fitting" means the model (the mathematical formula assumed to describe the data) does indeed explain patterns in the data; but if there's too much error between the model and the data, then it might get more wrong than right and therefore be useless.

The most important thing that well-trained statistical analysts know is which models can be used with which kinds of data, and how to analyze whether a dataset meets the models' assumptions in the first place (a model basically *is* just a bunch of assumptions, in a math formula). So if you're not trained in stats, you probably won't be able to weigh in on those questions. But with respect to actually interpreting the main model output which is the main evidence in most papers, all the authors are really doing in the "Findings" section is explaining a few key numbers, noting whether they're positive or negative (indicating a positive or negative *effect* of some variable on the variable they want to explain), and then checking to see if the error is small enough that they can be reasonably confident in that "effect."

Thus, a dirty little secret is that the *essential argument* of any statistical research paper can be understood (and even in an elementary way, questioned and critiqued) by looking at just two or three numbers in those big nasty tables.

    - the coefficients
    - the 

2. An annotated regression table

3. Questions to ask and things to look for
    - What is the unit?
    - What is the size of the effect?
    - Did they control for "x"?
    - Just look and see if it's in there!
    - What do they say about "errors"?
        + "residuals"
        + "outliers"
        + "missing data"
4. Email them
    - most of these people are well-paid by public institutions for the supposed brilliance of their research, so as a student you have every right to simply write them asking them questions. In my opinion, those who publish research *into* the public should be accountable to anyone from the public who might have some questions for them! Of course most people are busy, but most true intellectuals will try their best to answer your questions. If they don't respond, well then maybe that's shame on them and maybe we shouldn't trust them! And that's all the more reason to push forward on whatever you were curious or skeptical about!


[1]: Many of my colleages in quantitative social science will likely take great umbrage at this. We like to think of statistical research as a special forum too dangerous for non-statistical folks to explore. I will be told it's irresponsible of me to invite the unwashed masses into this domain where the untrained can do so much damage to themselves and others. But in my view, if statistics are so difficult and dangerous, that's exactly why everyone should be invited to the table. If inequalities in understanding are so severe that analytical havoc is wreaked, that is certainly not the fault of the untrained or their teacher, but rather the larger society which is perhaps too comfortable for status quo statisticians.

[2]: Professors engaged in statistical research are often partially responsible for this, because we often give the impression that statistical methods require a special analytical mastery, which we pretend to have! But the truth is that you don't need an mathematically sophisticated command of statistics to read, learn from, and engage research which may *use* even highly sophisticated methods.

[3]: A dirty little secret of quantitative social science is that there's actually huge variation in how well scholars really "understand" the statistical techniques they use. There's a saying that most economists are failed mathematicians, the point being that most people who *really* understand statistical modeling are in mathematics and statistics departments, and many social scientists are basically people who understand the stuff only well enough to type the right commands into a computer and describe the numerical output in basically "correct" ways!
